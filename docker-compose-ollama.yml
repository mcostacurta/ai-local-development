version: '3.8'

services:
  # Ollama - Local LLM Server
  ollama:
    build: 
      context: .
      dockerfile: Dockerfile-ollama
      args:
        - OLLAMA_TEXT_MODEL=gemma3
        - OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    tty: true
    networks:
      - ai_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    # Comment the line below if you have NVIDIA GPU
    runtime: runc

  # Qdrant - Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      - QDRANT__SERVICE__ENABLE_CORS=true
      - QDRANT__SERVICE__HTTP_HOST=0.0.0.0
      - QDRANT__SERVICE__GRPC_HOST=0.0.0.0
      - QDRANT__TLS__ENABLED=false
    networks:
      - ai_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Langflow - Visual Flow Builder for LLMs
  langflow:
    image: langflowai/langflow:latest
    container_name: langflow
    restart: unless-stopped
    ports:
      - "7860:7860"
    volumes:
      - langflow_data:/app/langflow
      - ./flows:/app/flows
    environment:
      - LANGFLOW_HOST=0.0.0.0
      - LANGFLOW_PORT=7860
      - LANGFLOW_WORKERS=1
      - LANGFLOW_SUPERUSER=admin
      - LANGFLOW_SUPERUSER_PASSWORD=admin123
      - LANGFLOW_SECRET_KEY=your-secret-key-here
      - LANGFLOW_DATABASE_URL=sqlite:///./langflow.db
      # Comment above line and uncoment bellow to use postgress 
      # - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow123@postgres:5432/langflow
      - LANGFLOW_AUTO_LOGIN=false
      - LANGFLOW_LOG_LEVEL=INFO
      # Configure connections to other services
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_GRPC_PORT=6334
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_USE_SSL=false
    networks:
      - ai_network
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_started
      # chromadb:
      #   condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: PostgreSQL for persistent storage (alternative to SQLite)
  # postgres:
  #   image: postgres:15-alpine
  #   container_name: postgres
  #   restart: unless-stopped
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #     - ./init-scripts:/docker-entrypoint-initdb.d
  #   environment:
  #     - POSTGRES_DB=langflow
  #     - POSTGRES_USER=langflow
  #     - POSTGRES_PASSWORD=langflow123
  #     - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
  #   networks:
  #     - ai_network
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U langflow -d langflow"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

networks:
  ai_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  ollama_data:
    driver: local
  langflow_data:
    driver: local
  qdrant_data:
    driver: local
  # postgres_data:
  #   driver: local